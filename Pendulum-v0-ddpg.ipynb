{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "tf.compat.v1.disable_eager_execution() # 关闭动态图机制\n",
    "\n",
    "class DDPGTrainer():\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_features, \n",
    "        n_actions, \n",
    "        sample_size=128, \n",
    "        tau=0.99, \n",
    "        gamma=0.95, \n",
    "        epsilon=1.0, \n",
    "        epsilon_decay=0.995, \n",
    "        epsilon_min=0.01, \n",
    "        a_lr=0.0001, \n",
    "        c_lr=0.001\n",
    "    ):\n",
    "        self.tau = tau\n",
    "        self.memory_buffer = deque(maxlen=4000)\n",
    "        self.sample_size = sample_size\n",
    "        self.gamma = gamma \n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.a_lr = a_lr\n",
    "        self.c_lr = c_lr\n",
    "        self.n_features = n_features\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.actor, self.critic = self.build_model()\n",
    "        self.target_actor, self.target_critic = self.build_model()\n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "        \n",
    "        \n",
    "    def build_model(self):\n",
    "        s_input = Input([self.n_features])\n",
    "        a_input = Input([1])\n",
    "        \n",
    "        # actor\n",
    "        x = Dense(units=40, activation='relu')(s_input)\n",
    "        x = Dense(units=40, activation='relu')(x)\n",
    "        x = Dense(units=1, activation='tanh')(x)\n",
    "        action = Lambda(lambda x: x * self.n_actions)(x)\n",
    "        actor = tf.keras.models.Model(inputs=s_input, outputs=action)\n",
    "        \n",
    "        # critic\n",
    "        x = K.concatenate([s_input, a_input], axis=-1)\n",
    "        x = Dense(40, activation='relu')(x)\n",
    "        x = Dense(40, activation='relu')(x)\n",
    "        q_a_value = Dense(1, activation='linear')(x)\n",
    "        critic = tf.keras.models.Model(inputs=[s_input, a_input], outputs=q_a_value)\n",
    "        \n",
    "        actor.add_loss(-critic([s_input, action])) # 最大化Q_a，注意有个负号\n",
    "        critic.trainable = False\n",
    "        actor.compile(optimizer=tf.keras.optimizers.Adam(self.a_lr))\n",
    "        critic.trainable = True\n",
    "        \n",
    "        actor.trainable = False\n",
    "        critic.trainable = True # 由于actor的计算图用到critic部分，actor.trainable变化会影响critic.trainable\n",
    "        critic.compile(optimizer=tf.keras.optimizers.Adam(self.c_lr), loss='mse')\n",
    "        actor.trainable = True\n",
    "        return actor, critic\n",
    "    \n",
    "\n",
    "    def OU(self, x, mu=0, theta=0.15, sigma=0.2):\n",
    "        return theta * (mu - x) + sigma * np.random.randn(1) # shape: [1]\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        action = self.actor.predict(state)[0][0] # shape: []\n",
    "        noise = max(self.epsilon, 0) * self.OU(action)\n",
    "        action = np.clip(action + noise, -self.n_actions, self.n_actions) # shape: [1]\n",
    "        return action\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        sample = (state, action, reward, next_state, done)\n",
    "        self.memory_buffer.append(sample)\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon >= self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def update_model(self):\n",
    "        samples = random.sample(self.memory_buffer, self.sample_size)\n",
    "        states = np.array([sample[0] for sample in samples])\n",
    "        actions = np.array([sample[1] for sample in samples])\n",
    "        rewards = np.array([sample[2] for sample in samples])\n",
    "        next_states = np.array([sample[3] for sample in samples])\n",
    "        dones = np.array([sample[4] for sample in samples])\n",
    "\n",
    "        next_actions = self.target_actor.predict(next_states)\n",
    "        q_a_next = self.target_critic.predict([next_states, next_actions]) # q_a_next.shape: [self.sample_size, 1]\n",
    "        y = rewards + self.gamma * q_a_next[:, 0] * ~dones  # y.shape: [self.sample_size]\n",
    "        self.critic.fit([states, actions], y[:, None], verbose=0) \n",
    "        self.actor.fit(states, verbose=0)\n",
    "        \n",
    "    def update_target_model(self):\n",
    "        actor_weights = self.actor.get_weights()\n",
    "        critic_weights = self.critic.get_weights()\n",
    "        actor_target_weights = self.target_actor.get_weights()\n",
    "        critic_target_weights = self.target_critic.get_weights()\n",
    "        for i in range(len(actor_target_weights)):\n",
    "            actor_target_weights[i] = actor_target_weights[i] * self.tau + (1 - self.tau) * actor_weights[i]\n",
    "        for i in range(len(critic_target_weights)):\n",
    "            critic_target_weights[i] = critic_target_weights[i] * self.tau + (1 - self.tau) * critic_weights[i]\n",
    "        self.target_actor.set_weights(actor_target_weights)\n",
    "        self.target_critic.set_weights(critic_target_weights)\n",
    "        \n",
    "    def save(self, checkpoint_path='pendulum'):\n",
    "        self.ckpt_manager.save()\n",
    "        with open(f'{checkpoint_path}/epsilon.pkl', 'wb') as f:\n",
    "            pickle.dump(self.epsilon, f)\n",
    "        \n",
    "    def load(self, checkpoint_path='pendulum'):\n",
    "        ckpt = tf.train.Checkpoint(\n",
    "            actor=self.actor,\n",
    "            critic=self.critic,\n",
    "            target_actor=self.target_actor,\n",
    "            target_critic=self.target_critic,\n",
    "            actor_optimizer = self.actor.optimizer,\n",
    "            critic_optimizer = self.critic.optimizer,\n",
    "        )\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
    "        \n",
    "        if os.path.exists(f'{checkpoint_path}/epsilon.pkl'):\n",
    "            with open(f'{checkpoint_path}/epsilon.pkl', 'rb') as f:\n",
    "                self.epsilon = pickle.load(f)\n",
    "                \n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            status = ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            status.run_restore_ops() # 关闭动态图后需要添加这句执行restore操作\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.compat.v1.InteractiveSession() # 关闭动态图后，ckpt_manager.save()需要有默认的session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output lambda_4 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to lambda_4.\n",
      "WARNING:tensorflow:Output lambda_5 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to lambda_5.\n",
      "episode0 total reward: -1132.2314667781802\n",
      "episode1 total reward: -1653.4862889361395\n",
      "episode2 total reward: -1600.8072680930939\n",
      "episode3 total reward: -1422.7313416379466\n",
      "episode4 total reward: -1273.8603840801647\n",
      "episode5 total reward: -1328.8167627819191\n",
      "episode6 total reward: -1368.0923254901684\n",
      "episode7 total reward: -1214.2838980258011\n",
      "episode8 total reward: -1195.7382097476486\n",
      "episode9 total reward: -1219.164374071276\n",
      "episode10 total reward: -1119.6288206332135\n",
      "episode11 total reward: -1165.3573075244553\n",
      "episode12 total reward: -1265.6987197976694\n",
      "episode13 total reward: -565.3520756898341\n",
      "episode14 total reward: -1224.7826103277896\n",
      "episode15 total reward: -1041.4816089924786\n",
      "episode16 total reward: -988.6489471709601\n",
      "episode17 total reward: -595.6588166046691\n",
      "episode18 total reward: -987.7924239019839\n",
      "episode19 total reward: -787.6139483971895\n",
      "episode20 total reward: -765.023713172175\n",
      "episode21 total reward: -738.7464172030428\n",
      "episode22 total reward: -760.0190769642367\n",
      "episode23 total reward: -570.3079128244926\n",
      "episode24 total reward: -855.6160888097712\n",
      "episode25 total reward: -632.6974077758475\n",
      "episode26 total reward: -863.4356616304091\n",
      "episode27 total reward: -756.821385071617\n",
      "episode28 total reward: -928.8335394524379\n",
      "episode29 total reward: -1193.1447759156808\n",
      "episode30 total reward: -882.3821571167987\n",
      "episode31 total reward: -1394.2346641804163\n",
      "episode32 total reward: -1167.4367046235727\n",
      "episode33 total reward: -1184.4848398367456\n",
      "episode34 total reward: -1011.7940149252054\n",
      "episode35 total reward: -1134.9231311371987\n",
      "episode36 total reward: -178.9210573206711\n",
      "episode37 total reward: -261.2634167135914\n",
      "episode38 total reward: -262.1147775596184\n",
      "episode39 total reward: -258.50807618263394\n",
      "episode40 total reward: -129.34416419404843\n",
      "episode41 total reward: -126.0201166407772\n",
      "episode42 total reward: -495.63979470413614\n",
      "episode43 total reward: -127.12455682101051\n",
      "episode44 total reward: -379.21650399487834\n",
      "episode45 total reward: -125.12389660748036\n",
      "episode46 total reward: -0.641422893929408\n",
      "episode47 total reward: -0.24294463522562543\n",
      "episode48 total reward: -535.7347620580181\n",
      "episode49 total reward: -363.0882628935516\n",
      "episode50 total reward: -132.02160559833504\n",
      "episode51 total reward: -355.1583577959267\n",
      "episode52 total reward: -132.57647716928463\n",
      "episode53 total reward: -129.09353920331415\n",
      "episode54 total reward: -1.0378958279199524\n",
      "episode55 total reward: -124.89133913236051\n",
      "episode56 total reward: -368.0910482580263\n",
      "episode57 total reward: -114.56635627997606\n",
      "episode58 total reward: -1384.4921589456217\n",
      "episode59 total reward: -126.82096141161692\n",
      "episode60 total reward: -128.1066103281241\n",
      "episode61 total reward: -245.33373435316128\n",
      "episode62 total reward: -121.1736005279026\n",
      "episode63 total reward: -115.51130688929986\n",
      "episode64 total reward: -0.4734403864829624\n",
      "episode65 total reward: -0.4311187099950623\n",
      "episode66 total reward: -123.15761973523759\n",
      "episode67 total reward: -504.155225316073\n",
      "episode68 total reward: -126.19672579614002\n",
      "episode69 total reward: -129.79374737096842\n",
      "episode70 total reward: -3.1432610520205304\n",
      "episode71 total reward: -3.0785294093814737\n",
      "episode72 total reward: -773.7432330041978\n",
      "episode73 total reward: -262.4072472343664\n",
      "episode74 total reward: -128.47098372197974\n",
      "episode75 total reward: -496.8730035648705\n",
      "episode76 total reward: -778.41585022954\n",
      "episode77 total reward: -247.06844158266793\n",
      "episode78 total reward: -115.96672922206726\n",
      "episode79 total reward: -500.7005830809083\n",
      "episode80 total reward: -254.06737564175887\n",
      "episode81 total reward: -235.8891382186613\n",
      "episode82 total reward: -359.7159766821002\n",
      "episode83 total reward: -248.76125800043093\n",
      "episode84 total reward: -119.10777782851085\n",
      "episode85 total reward: -4.4076878996915\n",
      "episode86 total reward: -122.98993680931802\n",
      "episode87 total reward: -5.475720595911261\n",
      "episode88 total reward: -9.68610231318631\n",
      "episode89 total reward: -126.20934880002613\n",
      "episode90 total reward: -122.18651743569454\n",
      "episode91 total reward: -254.55975046658548\n",
      "episode92 total reward: -121.95648061738353\n",
      "episode93 total reward: -249.74097586095434\n",
      "episode94 total reward: -12.575253986161073\n",
      "episode95 total reward: -15.702804841043688\n",
      "episode96 total reward: -366.57515041184655\n",
      "episode97 total reward: -262.58939056606647\n",
      "episode98 total reward: -271.3317844782284\n",
      "episode99 total reward: -136.0135054681874\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "model = DDPGTrainer(env.observation_space.shape[0], env.action_space.high[0])\n",
    "model.load()\n",
    "try:\n",
    "    for episode in range(100):\n",
    "        next_state = env.reset()\n",
    "        reward_sum = 0\n",
    "        for step in range(200):\n",
    "            env.render()\n",
    "            state = next_state\n",
    "            action = model.choose_action(state[None])\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward_sum += reward\n",
    "            model.store(state, action, reward, next_state, done)\n",
    "\n",
    "            if len(model.memory_buffer) > model.sample_size:\n",
    "                model.update_model()\n",
    "                model.update_target_model()\n",
    "                model.update_epsilon()\n",
    "        print(f'episode{episode} total reward: {reward_sum}')\n",
    "    model.save()\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
